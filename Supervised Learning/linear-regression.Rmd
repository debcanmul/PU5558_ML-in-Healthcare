---
title: "LinearRegression"
author: "DBlana"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load packages

Make sure you run install.packages for any packages you do not have installed first! In this case, you will probably need: install.packages("corrplot") and install.packages("vip")

```{r load-packages}

library(tidyverse)    # for general data science
library(tidymodels)   # for machine learning
library(corrplot)     # for visualising correlation matrices
library(vip)          # for variable importance plots

```

## Load data

Let's use the Scotland Vulnerability Resource (SVR) again. Remember that you can read more about it here: https://github.com/AbdnCHDS/Scotland_Vulnerability_Resource

Let's read in the data and have a look at it:

```{r read-data}

scot_data <- read_csv("Scotland-Vulnerability-Resource_v0.2.csv")

glimpse(scot_data)
  
```

The first thing we will do is remove the "postcode" variable, as all data is at datazone level:

```{r prepare-data-datazone}

DZ_data <- scot_data %>%
  select(-Postcode) %>%
  drop_na() %>%    # remove rows with missing values
  unique()         # keep unique rows
  
```

We will also remove geography names and population numbers, as we will not need them for prediction. 

```{r select-data}

DZ_data <- DZ_data %>%
  select(-Data_Zone,-Intermediate_Zone,-Council_area,-NHS_Health_Board_Region,-Total_population,-Working_age_population,-URname)

```

Let's see if we can predict the variable "DEPRESS", which is the proportion of the population being prescribed drugs for anxiety, depression or psychosis.

```{r plot-output}

DZ_data %>%
ggplot(aes(x = DEPRESS)) + 
  geom_histogram(bins = 30, col= "white")

```

The distribution of our outcome variable is relatively normal. If it was skewed, it would be a good idea to transform it. Have a look at the benefits of a log-transform, and R code to perform it here: https://www.tmwr.org/ames.html#exploring-important-features

## Data splitting

It is best practice to hold out some of our data for testing, to get a better estimate of how our model will perform on new data. This will allow us to assess if our model is overfitting. (This is not a big problem with linear regression, but we will practice with this anyway.)

We could do completely random sampling to partition the data, but it would be good to ensure that the output variable distribution is similar between the training and testing set. You can read more about this here: https://www.tmwr.org/splitting.html#splitting-methods

The code below puts 80% of it into a training dataset and 20% of it into a testing dataset, and chooses the individual data points so that the "DEPRESS" variable has similar distributions:

```{r data-splitting}

# Create the split
DZ_split <- DZ_data %>%
    initial_split(prop = 0.8,        # proportion in training set
                  strata = DEPRESS)  # ensure distribution of DEPRESS in the two datasets in similar

# Save the two datasets as objects
DZ_train <- training(DZ_split)
DZ_test <- testing(DZ_split)

```

Now we will put the testing dataset aside, and not touch it until we are ready to evaluate our model. We will use only the training data to choose predictors and train our model.

## Selection and preprocessing of predictors

Let's look at the correlation between the variables in our dataset, to help us select predictors.

```{r data-corr}

DZ_cor <- cor(DZ_train %>% select_if(is.numeric))  # we can only use numeric variables to calculate correlation
corrplot(DZ_cor, tl.cex = 0.5)

```

From this graph, we can see that "DEPRESS" is positively correlated with income, employment and qualification metrics, CIF, EMERG, and negatively correlated with attendance and most of the SIMD metrics. 

Let's use income and employment rate, CIF, EMERG, attendance, and SIMD health domain rank as our predictors. In a real problem, expert knowledge and understanding of causal relationships would be used to select suitable predictors.   

When linear regression is used for inference, it has four main assumptions that need to be satisfied to draw accurate conclusions about the relationships between predictors and outcome. If our only interest was to use linear regression for prediction, we would not need to worry about these assumptions. In practice, if these assumptions are violated, it means that linear regression is not a good model for our data, and the predictive performance will not be adequate. (And for healthcare applications, prediction should not be our only interest anyway, as we've discussed previously.)   

One of the main assumptions of linear regression is that there is a linear relationship between predictors and outcome. We can check this using scatterplots of DEPRESS vs each of the predictors. Here is the code for the SIMD health domain rank:

```{r scatterplot}

DZ_train %>%
  ggplot(aes(x = SIMD2020_Health_Domain_Rank, y = DEPRESS)) + 
  geom_point()

```

Try the same for the rest of our proposed predictors. We will also check this after we have fit our model, by plotting the residuals vs the fitted values.

Linear regression has three other assumptions:

- Independence of residuals: is each datazone independent of all others? This is probably not true, but we will not worry about this assumption in this example.  
- Normal distribution of residuals
- Equal variance of residuals

We will test these after we have fit our model.

Linear regression requires only numeric predictors, so if we had any categorical predictors we would need to convert them into numeric using dummy variables (https://www.tmwr.org/recipes.html#dummies). We would do this by adding:
step_dummy(all_nominal_predictors())
to the recipe.
But we don't need to worry about this with our dataset.

We will use step_nzv to remove variables that contain only a single value (although if we check we'll see we don't have any variables like that), and step_corr to remove variables that have large absolute correlations with other variables.

If we had missing data, we would also want to estimate them via imputation, there are several methods to do this: https://recipes.tidymodels.org/reference/#section-step-functions-imputation
In our case, we have already removed rows with missing data.

Read more about recommended pre-processing steps for each model here: https://www.tmwr.org/pre-proc-table.html

```{r specify-recipe}

simple_rec <- DZ_train %>%
  recipe(DEPRESS ~ Income_rate + Employment_rate + no_qualifications + CIF + EMERG + Attendance + SIMD2020_Health_Domain_Rank) %>%
  step_zv(all_predictors()) %>%
  step_corr(all_predictors())

```

Note that so far we have not used this recipe, we have just specified the steps. We will apply the recipe as part of our modelling workflow, after we have specified our model.

## Model specification

To specify our model, we need to make three decisions:

1. the model type (e.g. linear regression, random forest, etc.)
2. the model "mode" which is whether the model will be used for regression or classification. This is because some model types can support either. In our case, linear regression is only used for regression, so we do not need to specify the mode.
3. the model engine: this is the computational tool used to fit the model. We won't go into detail on this in this course, so you can use the default setting here. For linear regression, the default is "lm".  

Read more about linear regression here: https://parsnip.tidymodels.org/reference/linear_reg.html

```{r specify-model}

## A linear regression model specification with default settings
lm_spec <- linear_reg() 

```

Similar to the recipe specification, so far we have just specified our model, we have not fitted or evaluated it. Next, let's put the recipe and model together in our modelling workflow.

## Workflow

A "workflow" pairs a recipe and a model specification, so we can easily use them together. 

```{r specify-workflow}

DZ_wflow <-workflow() %>%
           add_recipe(simple_rec) %>%
           add_model(lm_spec)

```

## Model training

We are now ready to fit our model using our workflow. All we need to do is use the function fit() on our training data. This prepares the predictors according to our recipe, and trains the model using these predictors:

```{r train-model}

DZ_wflow_fit <- DZ_wflow %>%
    fit(data = DZ_train)

```

## Model inspection

We have fitted our model! Before we jump into doing predictions with our newly fitted model, let's check the model assumptions. First, we will extract the fitted model from our workflow:

```{r extract-model}

fitted_model <- extract_fit_engine(DZ_wflow_fit)

```

There are a number of diagnostic plots available for linear regression models:

1. Residuals vs Fitted. Here, a horizontal line indicates that the relationship between predictors and outcome is linear. 

```{r residuals-fitted}
plot(fitted_model, 1)
```

2. Normal Q-Q. This allows us to check whether the residuals are normally distributed, and we want the residuals to follow the straight line.

```{r q-q}
plot(fitted_model, 2)
```

3. Scale-Location. A horizontal line indicates that residuals have equal variance. 

```{r scale-location}
plot(fitted_model, 3)
```

We're relatively satisfied that the assumptions hold. Next, let's have a look at which variables are most important. The vip() function creates a bar plot of variable importance scores for each predictor variable in the model, here we are looking at the 5 most important variables:

```{r variable-importance}

DZ_wflow_fit %>% 
  extract_fit_parsnip() %>% 
  vip(num_features = 5)

```

The health domain rank of the SIMD appears to be the most important predictor of the proportion of the population being prescribed drugs for anxiety, depression or psychosis, among the variables we have included in our model. 

## Model evaluation

How good is our model at prediction? Here we need to consider which data and which metrics to use for evaluation.

The predict() function applies the recipe to the new data, then passes them to the fitted model.
The metrics() function calculates common performance metrics. For regression, it calculates:

1. the root mean squared error (rmse)
2. R squared (rsq)
3. the mean absolute error (mae)

Let's evaluate the model first on the training dataset. As we know, this is not a good idea, but let's see what we get:

```{r eval-training-data}

# Use the model for prediction on the same data it was trained on (bad idea!)
predicted_DEPRESS <- DZ_wflow_fit %>%
  predict(new_data = DZ_train)

# Add the predicted column to the dataset so we have everything in one dataframe
results_train <- DZ_train %>%
  bind_cols(predicted_DEPRESS) %>%
  rename(pred_DEPRESS = .pred)   # rename the predicted column

# Evaluate the performance
metrics(results_train, truth = DEPRESS, estimate = pred_DEPRESS)

```

Not bad, but not great, as our model is quite simple. Let’s evaluate how it performs on the testing data:

```{r eval-testing-data}

# Use the model for prediction on the testing data now
predicted_DEPRESS_test <- DZ_wflow_fit %>%
  predict(new_data = DZ_test)

# As before, add the predicted column to the dataset so we have everything in one dataframe
results_test <- DZ_test %>%
  bind_cols(predicted_DEPRESS_test) %>%
  rename(pred_DEPRESS = .pred)   # rename the predicted column

# Evaluate the performance
metrics(results_test, truth = DEPRESS, estimate = pred_DEPRESS)

```
As you can see, the performance metrics are not that different between the training and testing datasets. 

The difference is small because our simple linear regression model has low variance (and high bias), which means that it is not prone to overfitting. In the next worksheet, we will look at the random forest algorithm, which behaves differently.

Finally, let's plot the actual and predicted values of "DEPRESS" so we can visually inspect the model fit.

```{r plot-model-prediction}

results_test %>%
  ggplot(aes(x = DEPRESS, y = pred_DEPRESS)) +
  geom_abline(intercept=0, slope=1) +  # we want data to fall close to this line
  geom_point() +
  xlab("Actual outcome values") + 
  ylab("Predicted outcome values")

```

## Practice

1. Repeat this analysis for different predictors, or a different outcome. Remember that if you use categorical predictors, you will need to convert them to numeric first. 

2. You have been given a second dataset to practice with: this contains estimates of the percentage of body fat determined by underwater weighing, and various body circumference measurements for 252 men. Can you predict body fat from a combination of the variables in the dataset? The dataset came from Kaggle, which is a great platform for sharing machine learning datasets and code. You can read more about what is included in the dataset here: https://www.kaggle.com/fedesoriano/body-fat-prediction-dataset
